---
title: "HW3"
author: "Lauren Stover"
date: "3/31/2021"
output:
  pdf_document: default
  html_document: default
---

# ECO 395M: Exercises 3

Due date: Friday, April 9, 9 AM US Central Time  

## What causes what?  

First, listen to [this podcast from Planet Money.](https://www.npr.org/sections/money/2013/04/23/178635250/episode-453-what-causes-what)  Then use your knowledge of statistical learning to answer the following questions.

1. Why can’t I just get data from a few different cities and run the regression of “Crime” on “Police” to understand how more cops in the streets affect crime? (“Crime” refers to some measure of crime rate and “Police” measures the number of cops in a city.)  
-You can't just get the data from a few different cities and run crime on police because the number of police officers dispatched at any given time may be due to other reasons. It may be due to a terrorist threat in D.c., it may be due to a holiday in a major city so they dispatch more officers on any given day. 


2. How were the researchers from UPenn able to isolate this effect? Briefly describe their approach and discuss their result in the “Table 2” below, from the researchers' paper.  


-The researchers were able to isolate the effect of the high alert by looking at the midday ridership. By controlling for midday ridership, one could understand if due to the high alert days there was less crime because there were less civilians. As seen in Table 2, the civilian ridership was similar on days when high alert was issue and therefore ridership was significant at the 1 percent level while high alert was significant at the 5 percent level. 

3. Why did they have to control for Metro ridership? What was that trying to capture?   
-The researchers controlled for metro ridership because they suspected on days where more police are dispatched within D.C. due to higher terrorist threats, there may be less civilians which could contribute to less victims for crimes. The Metro ridership indicated that civilians on these higher terrorist threat days were likely similar in levels due to the Metro ridership. 

4. Below I am showing you "Table 4" from the researchers' paper.  Just focus
on the first column of the table. Can you describe the model being estimated here?
What is the conclusion?


The model being estimated here is a linear model that includes the daily total number of crimes being predicted by high alert interacted with the dummy variable that is district 1, high alert being interacted with other distracts, a log of the midday ridership and the constant term that absorbs unobservable effects. Here we can see that the correlation between the daily rate of crimes and the high alert interacted with crime in the dummy variable is significant at the 1% level. This means that when there is a high alert in this district, the total daily amount of crimes is extremely correlated. The log of midday ridership is correlated with daily crime at the 5% level, which makes sense. As the number of civilians increases, crime will be able to increase. 

## Predictive model building: green certification   

Consider the data set on green buildings in [greenbuildings.csv](../data/greenbuildings.csv).  This  contains data on 7,894 commercial rental properties from across the United States. Of these, 685 properties have been awarded either LEED or EnergyStar certification as a green building.  Here is a list of the variables:

- CS.PropertyID:  the building's unique identifier in the database.  
- cluster:  an identifier for the building cluster, with each cluster containing one green-certified building and at least one other non-green-certified building within a quarter-mile radius of the cluster center.  
- size:  the total square footage of available rental space in the building.  
- empl.gr:  the year-on-year growth rate in employment in the building's geographic region.  
- Rent:  the rent charged to tenants in the building, in dollars per square foot per calendar year.  
- leasing.rate:  a measure of occupancy; the fraction of the building's available space currently under lease.  
- stories:  the height of the building in stories.  
- age:  the age of the building in years.  
- renovated:  whether the building has undergone substantial renovations during its lifetime.  
- class.a, class.b:  indicators for two classes of building quality (the third is Class C).  These are relative classifications within a specific market.  Class A buildings are generally the highest-quality properties in a given market.  Class B buildings are a notch down, but still of reasonable quality.  Class C buildings are the least desirable properties in a given market.  
- green.rating:  an indicator for whether the building is either LEED- or EnergyStar-certified.  
- LEED, Energystar:  indicators for the two specific kinds of green certifications.  
- net:  an indicator as to whether the rent is quoted on a "net contract" basis.  Tenants with net-rental contracts pay their own utility costs, which are otherwise included in the quoted rental price.  
- amenities:  an indicator of whether at least one of the following amenities is available on-site: bank, convenience store, dry cleaner, restaurant, retail shops, fitness center.  
- cd.total.07:  number of cooling degree days in the building's region in 2007.  A degree day is a measure of demand for energy; higher values mean greater demand.  Cooling degree days are measured relative to a baseline outdoor temperature, below which a building needs no cooling.  
- hd.total07:  number of heating degree days in the building's region in 2007.  Heating degree days are also measured relative to a baseline outdoor temperature, above which a building needs no heating.  
- total.dd.07:  the total number of degree days (either heating or cooling) in the building's region in 2007.  
- Precipitation:  annual precipitation in inches in the building's geographic region.
- Gas.Costs:  a measure of how much natural gas costs in the building's geographic region.  
- Electricity.Costs:  a measure of how much electricity costs in the building's geographic region.  
- City_Market_Rent:  a measure of average rent per square-foot per calendar year in the building's local market.    


Your goal is to build the best predictive model possible for _revenue per square foot per calendar year_, and to use this model to quantify the average change in rental income per square foot (whether in absolute or percentage terms) associated with green certification, holding other features of the building constant.  Note that revenue per square foot per year is the product of two terms: rent and leasing_rate!  This reflects the fact that, for example, high-rent buildings with low occupancy may not actually bring in as much revenue as lower-rent buildings with higher occupancy.  

You can choose whether to consider LEED and EnergyStar separately or to collapse them into a single "green certified" category.  You can use any modeling approaches in your toolkit (regression, variable selection, trees, etc), and you should also feel free to do any feature engineering you think helps improve the model.  Just make sure to explain what you've done.  

Write a short report detailing your methods, modeling choice, and conclusions, following the [report-writing guidelines posted on the website](https://jgscott.github.io/teaching/writeups/write_ups/).  


### Report
#Overview:
We are give the task of determining how revenue per square foot per calendar year changes for apartments buildings that are green certified, holding other features constant. The idea is that green certified buildings should be more desirable as they save money in terms of utilities for the tenants and save the landlords money in basic electricity costs as well. 

#Data and the model:
We are looking at data that contains data on 7,894 commercial rental properties from across the United States. Of these, 685 properties, or about 8.6%, have been awarded either LEED or EnergyStar certification as a green building.


```{r, echo=FALSE, error=FALSE}
library(tidyverse)
library(ggplot2)
library(naivebayes)
library(modelr)
library(rsample)
library(foreach)
library(caret)
library(parallel)
library(mosaic)
library(foreach)
greenbuildings <- read_csv("~/Documents/UTX/DataMining/ECO395M/data/greenbuildings.csv")

```

First, I will create a variable 'revenue.' This variable will represent the revenue per square foot per year that a building could obtain. I will generate it by multiplying the rent per square foot per year by the building leasing rate. 
```{r, echo=FALSE}
greenbuildings = greenbuildings %>%
  mutate(revenue = (Rent*leasing_rate))
```

Now, I will split the data into a training and testing set. I will also fit a linear regression model that includes all the possible variables as predictors for revenue to ensure all variable are controlled for. 

```{r, echo=FALSE, error=FALSE}
green_split = initial_split(greenbuildings, prop = 0.8)
green_train = training(green_split)
green_test = testing(green_split)

# Fit to the training data
lm1g = lm(revenue ~ CS_PropertyID+cluster+size+empl_gr+Rent+leasing_rate+stories+age+renovated+class_a+class_b+green_rating+LEED+Energystar+net+amenities+cd_total_07+hd_total07+total_dd_07+Precipitation+Gas_Costs+Electricity_Costs+cluster_rent, data=green_train)

coef(lm1g) %>% round(0)

# Predictions out of sample
# Root mean squared error
rmse(lm1g, green_test)
```
As we can see above there are some variables that have zero use reported.. We will remove these variables and re-run the linear regression from above. 

```{r,echo=FALSE}
# Fit to the training data
lm2g = lm(revenue ~ Rent+leasing_rate+age+renovated+class_a+class_b+green_rating+LEED+Energystar+net+amenities+Precipitation+Gas_Costs+Electricity_Costs+cluster_rent, data=green_train)

coef(lm2g) %>% round(0)

# Predictions out of sample
# Root mean squared error
rmse(lm2g, green_test)
```
The RMSE decreased only slightly, likely due to variance in the data set. Moving forward, we will utilize this model and perform various feature engineering techniques in order to enhance the model.

First, I will begin by interacting the age and renovated variables. 

```{r, echo=FALSE}
# Fit to the training data
lm3g = lm(revenue ~Rent+leasing_rate+age+renovated+class_a+class_b+green_rating+LEED+Energystar+net+amenities+Precipitation+Gas_Costs+Electricity_Costs+cluster_rent+ age:renovated, data=green_train)

coef(lm3g) %>% round(0)

# Predictions out of sample
# Root mean squared error
rmse(lm3g, green_test)
```
As we can see, this interaction has made the model worse off. We will remove this interaction and now focus on interacting the amenities and electricity costs variables. 

```{r, echo=FALSE}
# Fit to the training data
lm4g = lm(revenue ~Rent+leasing_rate+age+renovated+class_a+class_b+green_rating+LEED+Energystar+net+amenities+Precipitation+Gas_Costs+Electricity_Costs+cluster_rent+ amenities:Electricity_Costs, data=green_train)

coef(lm4g) %>% round(0)

# Predictions out of sample
# Root mean squared error
rmse(lm4g, green_test)
```
This interaction improved the model by about 2 points. We will now look into a few more interactions to see if the model can be fit any better.

```{r, echo=FALSE}
# Fit to the training data
lm4g = lm(revenue ~Rent+leasing_rate+age+renovated+class_a+class_b+green_rating+LEED+Energystar+net+amenities+Precipitation+Gas_Costs+Electricity_Costs+cluster_rent+ amenities:Rent+ amenities:Electricity_Costs, data=green_train)

coef(lm4g) %>% round(0)

# Predictions out of sample
# Root mean squared error
rmse(lm4g, green_test)
```

The interaction between amenities and electricity costs decreases the RMSE by about 6 more points. This is a fairly large change at this level. 

#Results
As we can see, the RMSE decreased each model until it resulted with an RMSE of 309.1046. This means that this linear model is the best prediction of revenue per year for the different apartment buildings.

If we calculate the marginal effects of our model, one can see that the change in the rent and leasing rate is relatively the same across the model. 
```{r, echo=FALSE}
library(margins)
me=marginal_effects(lm4g, greenbuildings, variables=NULL)
head(me)
```
Now I will plot the predicted values of revenue based on whether or not a building is either LEED certified or EnergyStar certified. 
```{r, echo=FALSE}
library(sjPlot)
library(ggplot2)
plot_model(lm4g, type = "pred", terms = "green_rating")
```
From the plot above, one can see that as the green rating goes from 0 to 1, the predicted revenue value decreases from slightly above 2400 to slightly below 2350. This would indicate that a building that is either LEED certified or EnergyStar certified does not create a higher renal income per square foot. 

#Conclusion
In conclusion, this model displayed that a building that is either LEED certified or EnergyStar certified does not create a higher renal income per square foot. One explanation could be due to the initial investment, landlords had to increase the rental rates in the green buildings. Due to the increase in these rental rates, tenants were more likely to choose a building that was not green certified. This means the tenants value the rental rate over being green so to speak. 


## Predictive model building: California housing  

The data in `CAhousing.csv` containts data at the census-tract level on residential housing in the state of California.  Each row is a [census tract](https://libguides.lib.msu.edu/tracts), and the columns are as follows:  

- longitude, latitude: coordinates of the geographic centroid of the census tract  
- housingMedianAge: median age in years of all residential households in the census tract  
- population: total population of the tract  
- households: total number of households in the tract.  
- totalRooms, totalBedrooms: total number of rooms and bedrooms for households in the tract.  NOTE: these are _totals_, not averages.  Consider standardizing by households.  
- medianIncome: median household income in USD for all households in the tract.  
- medianHouseValue: median market value of all households in the tract.  

Your task is to build the best predictive model you can for `medianHouseValue`, using the other available features.  Write a short report detailing your methods.  Make sure your report includes an estimate for the overall out-of-sample accuracy of your proposed model.  Also include three figures:  

- a plot of the original data, using a color scale to show medianHouseValue (or log medianHouseValue) versus longitude (x) and latitude (y).  
- a plot of your model's predictions of medianHouseValue (or log medianHouseValue) versus longitude (x) and latitude (y).  
- a plot of your model's errors/residuals (or log residuals) versus longitude (x) and latitude (y).

You can get nearly full credit (but not 100%) without a mapping package, i.e. just treating longitude and latitude as generic x/y coordinates.  But a modest number of points will be reserved for those who can successfully show these plots in a visually pleasing fashion on an _actual map of California_.  This will entail learning how to use an R package capable of making maps.  (We haven't covered this in class, but a major part of being a data scientist is learning how to use new software tools and libraries "on the fly" like this.)  I recommend `ggmap` as a good starting point, but you can use whatever R tools you want here.  


#Overview
I will be attempting to predict median house value in various areas of the state of California. The data used has several factor variables that can contribute to a houses value. 

#Data and Model
I used data from a census-track level for housing in the state of California. 
```{r, echo=FALSE}
library(tidyverse)
library(ggplot2)
library(naivebayes)
library(modelr)
library(rsample)
library(foreach)
library(caret)
library(parallel)
library(mosaic)
library(foreach)
housing <- read_csv("~/Documents/UTX/DataMining/ECO395M/data/CAhousing.csv")
```

Below is a map of the state of California with the median house value mapped. We can see that the median housing price increases the further toward the coast a house is. 
```{r, echo=FALSE}
library(ggplot2)
library(ggmap)
library(maps)
library(mapdata)
library(dplyr)
library(viridis)

housing <- housing %>%
    mutate(long = longitude,
           lat  = latitude)

states <- map_data("state")
cali <- subset(states, region %in% c("california"))
head(cali)

ggplot(cali, aes(x = long, y = lat, group = group)) + 
  geom_polygon(fill = "lightblue", color = "black") +  
  geom_point(data = housing, aes(group = NULL, color = medianHouseValue)) +
  theme_void()
  

```
Now I will begin to model the home prices to attempt to develop the best model for predictive behavior.

First, the total bedrooms and total rooms variables are for an entire region, not per household. We need to divide each by households to standardize the variables 
```{r}
housing$rooms <- housing$totalRooms / housing$households
housing$bedrooms <- housing$totalBedrooms / housing$households
```

Next, we can begin modeling. I will first split the data into a training set and a testing set of data. Then I will fit a linear regression model to the training data. I will start with an extremely basic model that utilizes the variables longitude, latitude, and households to predict the median house value. 
```{r, echo=FALSE}
# Split into training and testing sets
housing_split = initial_split(housing, prop = 0.8)
housing_train = training(housing_split)
housing_test = testing(housing_split)
	
# Fit to the training data
lm1 = lm(medianHouseValue ~ long +lat+households, data=housing_train)

coef(lm1) %>% round(0)

# Predictions out of sample
# Root mean squared error
rmse(lm1, housing_test)
```
One can see above the basic model reports an extremely high RMSE. I will now work on improving the model through feature engineering.
First to improve the model, I will add the variables for housing median age, median income, and rooms. 

```{r, echo=FALSE}
# Fit to the training data
lm2 = lm(medianHouseValue ~ long + lat + households + rooms + medianIncome + housingMedianAge, data=housing_train)

coef(lm2) %>% round(0)

# Predictions out of sample
# Root mean squared error
rmse(lm2, housing_test)
```
We can see already the RMSE has been greatly reduced just by the introduction of a few extra variables. I will add bedrooms to see if the RMSE can be reduced further.
```{r, echo=FALSE}
# Fit to the training data
lm3 = lm(medianHouseValue ~ long + lat + households + rooms + medianIncome + housingMedianAge +bedrooms, data=housing_train)

coef(lm3) %>% round(0)

# Predictions out of sample
# Root mean squared error
rmse(lm3, housing_test)
```
One can see adding bedrooms reduces the RMSE, but by only about 350. While this is still an improvement, it is not significant. There is one last variable, population, we can add to the basic model. I originally theorized that this variable would cause colinearity in the model so I intentionally omitted it. I will include this variable now to test if my theory is correct.

```{r, echo=FALSE}
# Fit to the training data
lm4 = lm(medianHouseValue ~ long + lat + households + rooms + medianIncome + housingMedianAge +bedrooms +population, data=housing_train)

coef(lm4) %>% round(0)

# Predictions out of sample
# Root mean squared error
rmse(lm4, housing_test)
```
My theory was proven incorrect, and by including population the RMSE was reduced by about 3000 points. Moving forward, I will include it in the model.

Now I will square the previous linear model that includes all variables to see if it will improve the RMSE.
```{r, echo=FALSE}
lm5 = lm(medianHouseValue ~ (long + lat + households + rooms + medianIncome + housingMedianAge +bedrooms +population) ^2, data=housing_train)

coef(lm5) %>% round(0)

# Predictions out of sample
# Root mean squared error
rmse(lm5, housing_test)
```
One can see that the RMSE was improved by about 300 points. 

Now we can try several different feature engineering methods to potentially improve the model further. We will begin by exploring whether interacting two variables can make a difference.

```{r, echo=FALSE}
lm6 = lm(medianHouseValue ~ (long + lat + households + rooms + medianIncome + housingMedianAge +bedrooms +population +households:medianIncome) ^2, data=housing_train)

coef(lm6) %>% round(0)

# Predictions out of sample
# Root mean squared error
rmse(lm6, housing_test)
```
We can see an improvement of about 38 points. Let's try to interact a few other variables.

```{r, echo=FALSE}
lm7 = lm(medianHouseValue ~ (long + lat + households + rooms + medianIncome + housingMedianAge +bedrooms +population +households:medianIncome + medianIncome:housingMedianAge + medianIncome:rooms) ^2, data=housing_train)

coef(lm7) %>% round(0)

# Predictions out of sample
# Root mean squared error
rmse(lm7, housing_test)
```
One can see that by adding a few other interaction variables, we were able to achieve the lowest RMSE yet. 

```{r, echo=FALSE}
housing = housing %>%
  mutate(lm7predict = predict(lm7, newdata=housing))
```

#Results
Let's go ahead and re-plot the original map but now with the new predicted values from the best predicting linear model.
```{r, echo=FALSE}
library(ggplot2)
library(ggmap)
library(maps)
library(mapdata)
library(dplyr)
library(viridis)

housing <- housing %>%
    mutate(long = longitude,
           lat  = latitude)

states <- map_data("state")
cali <- subset(states, region %in% c("california"))
head(cali)

ggplot(cali, aes(x = long, y = lat, group = group)) + 
  geom_polygon(fill = "lightblue", color = "black") +  
  geom_point(data = housing, aes(group = NULL, color = lm7predict)) +
  theme_void()
```
It appears our model does a good job of predicting a similar result to those of the original plot. One can see that the houses as before are more expensive near the coast than they ar inland. there is an area at the elbow of the east side of the state that is showing similar values to that of the original plot. 

I will now plot the errors over the state of California. Here I defined errors as the difference between the actual values for median house value and the predicted values for the seventh linear model. 
```{r, echo=FALSE}
housing <- housing%>%
  mutate(errors=(medianHouseValue-lm7predict))

library(ggplot2)
library(ggmap)
library(maps)
library(mapdata)
library(dplyr)
library(viridis)

housing <- housing %>%
    mutate(long = longitude,
           lat  = latitude)

states <- map_data("state")
cali <- subset(states, region %in% c("california"))
head(cali)

ggplot(cali, aes(x = long, y = lat, group = group)) + 
  geom_polygon(fill = "lightblue", color = "black") +  
  geom_point(data = housing, aes(group = NULL, color = errors)) +
  theme_void()

```
The errors are centered around zero, indicating our model does a good job of predicting the median house value. 


#Conclusion
In conclusion the linear model I built did a good job of predicting the median house value for a home in the state of California. Based on the lowest RMSE possible and a map of the errors that would indicate most are around zero, the model predicted similar values for median housing prices to that of the original data. 

